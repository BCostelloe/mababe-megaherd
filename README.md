# Counting the Mababe Mega-Herd

<p align="center">
<img src="images/cover_image.png" height="500px">
</p>


This repository contains the code necessary to recreate the semi-automated counting procedure described in Bennitt *et al.* (in prep). If you find the code or the paper useful in your own studies, please cite this project:

Bennitt, E., Costelloe, B., Koger, B., Wielgus, E., Masunga, G., Caron, A. (In prep) Conservation counts: combining artificial and human intelligence to document the largest ever-counted Cape buffalo mega-herd.

## Data availability
The data required to run this code and the outputs generated by the code can be downloaded from Edmond (**LINK PENDING**). To run the code, download and unzip the mababe-megaherd-data file from the Edmond repository, and move the "data", "figure_videos", and "output" folders into the project folder (that contains this README file). Data file structure and contents are as follows:
- **data**
    - **annotations**
        - **annotated_images** - folder containing images that we annotated for model training.
        - **coco_megaherd_export-2023-04-20T07_24_16.846Z.json** - Contains the annotations generated in Labelbox, converted to COCO format.
        - **export-2023-04-20T07_24_16.846Z.json** - Contains the annotations generated in Labelbox format.
        - **random_train_megaherd.json** - Contains the annotations used as the training set for the detection model.
        - **random_val_megaherd.json** - Contains the annotations used as the validation set for the detection model.
    - **cropped_frames_8fps** - folder containing all the video frames analyzed, cropped to the "count area"
    - **raw_tracks2** - contains the 4868 trajectories generated by the initial automated tracking procedure. 
    - **retained_tracks.npy** - contains the 2972 trajectories retained after we removed partial trajectories.
- **figure_videos** - videos generated during processing and analysis.
    - **manual_review_clips** - folder containing the video clips used for manual review and count correction, including Supplementary Video 3 (filename output_video_1_6.mp4).
    - **review_video.mp4** - the video used to create the manual review clips
    - **supplementary_video1.mp4** - a video showing the 4868 initially-generated trajectories
    - **supplementary_video2.mp4** - a video showing the 2972 retained trajectories, which flash when they cross a super-imposed count line.
- **output:** folder containing the output from the detection model training and inference steps

## Processing steps

**1. Extract and crop video frames.** We cropped the original video provided by Netflix to a "count area" in the bottom left corner of the frame, and saved every third frame (thereby downsampling the video to 8 frames per second) using [Notebook 1](1_crop_downsample_video.ipynb). The original video is proprietary imagery and therefore not included in the dataset. The cropped and extracted frames used for all analyses are provided in the folder "cropped_frames_8fps" in the dataset.

**2. Creating training and validation image sets.** We extracted randomly placed crops from randomly-selected frames generated in Step 1. We then annotated these images using [Labelbox](https://labelbox.com/), and randomly assigned annotated images to the training and validation sets. For these steps, we used code from Koger *et al* (2023), which is available [here](https://github.com/benkoger/overhead-video-worked-examples). The images, annotations, and JSON files specifying the image sets are provided in the dataset in the data/annotations folder.

**3. Train detection model.** We used code from Koger *et al.* (2023) to train a FasterRCNN object detection model with a resnet-50-fpn backbone. The original notebook is available [here](https://github.com/benkoger/overhead-video-worked-examples/blob/main/geladas/detection/model-training/train_gelada_detection.ipynb). The config file and results of this step are provided in the dataset in the output folder.

**4. Process cropped video frames.** We used the trained model to process the cropped video frames using code from Koger *et al.* (2023), available [here](https://github.com/benkoger/overhead-video-worked-examples/blob/main/geladas/detection/inference/process-video.ipynb). The detections generated by this process are provided in the [inference folder](output/megaherd-random-plateau-LRscheduler-cropped-color-aug-new-dataloader_6-6-23_maxiter-9000_lr-0.0019_detectPerIm-800_minsize-0_batchsize-8_nms-0.5/inference/).

**5. Generate initial trajectories.** We used code from Koger *et al.* (2023), available [here](https://github.com/benkoger/overhead-video-worked-examples/blob/main/geladas/tracking/detections_to_tracks.ipynb), to connect the detections generated in Step 4 into movement trajectories. We used the following tracking parameters to generate the trajectories:
        
        'max_distance_threshold': 50,
        'max_distance_threshold_noise': 30,
        'min_distance_threshold': 0,
        'max_unseen_time': 10,
        'min_new_track_distance': 5,
        'new_track_noise': 10,
        'max_noise_value': 11
We set min_length_threshold to 10, to remove any trajectories shorter than 10 frames, and retained only trajectories with a confidence score of at least 0.2. The resulting trajectories are provided in the dataset (raw_tracks2.npy). Code for producing a video with these trajectories overlaid (Supplementary Video 1, provided in the figure_videos folder in the dataset) is provided in [Notebook 2](2_make_tracked_videos.ipynb).

**6. Remove partial trajectories.** From the initial set of 4868 trajectories, we eliminated any trajectories that did not fully cross a horizontal buffer area defined as 5 pixels above and below a "countline" drawn horizontally across the count area. See [Notebook 3](3_remove_partial_tracks.ipynb). The remaining 2972 trajectories are provided in the dataset (data/retained_tracks.npy). [Notebook 2](2_make_tracked_videos.ipynb) can be used to make a video showing the retained trajectories, which "flash" as they cross the countline (Supplementary Video 2, provided in the figures_videos folder of the dataset).

**7. Generate video clips for manual review.** We use [Notebook 2](2_make_tracked_videos.ipynb) to create a "review video" (fiure_videos/review_video.mp4 in the dataset) showing the retained tracks, which flash as they go over the countline, as well as vertical red lines dividing the video into horizontal sections. We then use [Notebook 4](4_make_validation_videos.ipynb) to split this video into 5 horizontal sections, each of which is then split into 20-second segments. These videos (e.g. Supplementary Video 3) were then manually reviewed by the authors to correct the buffalo count initially generated by the detection and tracking process. The generated video clips are provided in the dataset (figures_videos/manual_review_clips).

Please see the manuscript for details regarding the manual review process that yielded the final herd size count of 3682 buffalo.

**References**
1. Koger, B., Deshpande, A., Kerby, J.T., Graving, J.M., Costelloe, B.R., Couzin, I.D. (2023) Quantifying the movement, behavior and environmental context of group-living animals using drones and computer vision. *Journal of Animal Ecology* 92: 1351-1371.