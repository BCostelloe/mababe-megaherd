# Counting the Mababe Mega-Herd

<p align="center">
<img src="images/cover_image.png" height="500px">
</p>


This repository contains the code necessary to recreate the semi-automated counting procedure described in Bennitt *et al.* (in prep). If you find the code or the paper useful in your own studies, please cite this project:

Bennitt, E., Costelloe, B., Koger, B., Wielgus, E., Masunga, G., Caron, A. (In prep) Combining artificial and human intelligence to document the largest ever-counted Cape buffalo mega-herd.

## Data availability
The data required to run this code and the outputs generated by the code can be downloaded from Edmond (**LINK PENDING**). To run the code, download and unzip the mababe-megaherd-data file from the Edmond repository, and move the "data", "figures_videos", and "output" folders into the project folder (that contains this README file). Data file structure and contents are as follows:
- **data**
    - **annotations**
        - **annotated_images** - folder containing images that we annotated for model training.
        - **coco_megaherd_export-2023-04-20T07_24_16.846Z.json** - Contains the annotations generated in Labelbox, converted to COCO format.
        - **export-2023-04-20T07_24_16.846Z.json** - Contains the annotations generated in Labelbox format.
        - **random_train_megaherd.json** - Contains the annotations used as the training set for the detection model.
        - **random_val_megaherd.json** - Contains the annotations used as the validation set for the detection model.
    - **cropped_frames_8fps** - folder containing all the video frames analyzed, cropped to the "count area".
    - **double_observer.csv** - a csv file containing the data for the double-observer analysis of the second round of manual count review.
    - **manual_count_results.csv** - a csv file containing the final manual count for each 20-second video clip.
    - **raw_tracks2** - contains the 4868 trajectories generated by the initial automated tracking procedure. 
    - **retained_tracks.npy** - contains the 2972 trajectories retained after we removed partial trajectories.
- **figures_videos** - videos generated during processing and analysis.
    - **manual_review_clips** - folder containing the video clips used for manual review and count correction, including Supplementary Video 3 (filename output_video_1_6.mp4).
    - **review_video.mp4** - the video used to create the manual review clips
    - **supplementary_video1.mp4** - a video showing the 4868 initially-generated trajectories
    - **supplementary_video2.mp4** - a video showing the 2972 retained trajectories, which flash when they cross a super-imposed count line.
- **output:** folder containing the output from the detection model training and inference steps

## Processing steps

**1. Extract and crop video frames.** We cropped the original video provided by Netflix to a "count area" in the bottom left corner of the frame, and saved every third frame (thereby downsampling the video to 8 frames per second) using [Notebook 1](1_crop_downsample_video.ipynb). The original video is proprietary imagery and therefore not included in the dataset. The cropped and extracted frames used for all analyses are provided in the folder "cropped_frames_8fps" in the dataset.

**2. Creating training and validation image sets.** We extracted randomly placed crops from randomly-selected frames generated in Step 1. We then annotated these images using [Labelbox](https://labelbox.com/), and randomly assigned annotated images to the training and validation sets. For these steps, we used code from Koger *et al* (2023), which is available [here](https://github.com/benkoger/overhead-video-worked-examples). The images, annotations, and JSON files specifying the image sets are provided in the dataset in the data/annotations folder.

**3. Train detection model.** We used code from Koger *et al.* (2023) to train a FasterRCNN object detection model with a resnet-50-fpn backbone. The original notebook is available [here](https://github.com/benkoger/overhead-video-worked-examples/blob/main/geladas/detection/model-training/train_gelada_detection.ipynb). The config file and results of this step are provided in the dataset in the output folder.

**4. Evaluate detection model.** We adapted code from Koger *et al.* (2023) to evaluate the performance of the detection model. This code, including code to generate Figure 4 in the main text, is in [Notebook 2](2_evaluate_detection_model.ipynb).

**4. Process cropped video frames.** We used the trained model to process the cropped video frames using code from Koger *et al.* (2023), available [here](https://github.com/benkoger/overhead-video-worked-examples/blob/main/geladas/detection/inference/process-video.ipynb). The detections generated by this process are provided in the [inference folder](output/megaherd-random-plateau-LRscheduler-cropped-color-aug-new-dataloader_6-6-23_maxiter-9000_lr-0.0019_detectPerIm-800_minsize-0_batchsize-8_nms-0.5/inference/).

**5. Generate initial trajectories.** We used code from Koger *et al.* (2023), available [here](https://github.com/benkoger/overhead-video-worked-examples/blob/main/geladas/tracking/detections_to_tracks.ipynb), to connect the detections generated in Step 4 into movement trajectories. We used the following tracking parameters to generate the trajectories:
        
        'max_distance_threshold': 50,
        'max_distance_threshold_noise': 30,
        'min_distance_threshold': 0,
        'max_unseen_time': 10,
        'min_new_track_distance': 5,
        'new_track_noise': 10,
        'max_noise_value': 11
We set min_length_threshold to 10, to remove any trajectories shorter than 10 frames, and retained only trajectories with a confidence score of at least 0.2. The resulting trajectories are provided in the dataset (raw_tracks2.npy). Code for producing a video with these trajectories overlaid (Supplementary Video 1, provided in the figures_videos folder in the dataset) is provided in [Notebook 3](3_make_tracked_videos.ipynb).

**6. Remove partial trajectories.** From the initial set of 4868 trajectories, we eliminated any trajectories that did not fully cross a horizontal buffer area defined as 5 pixels above and below a "countline" drawn horizontally across the count area. See [Notebook 4](4_remove_partial_tracks.ipynb). The remaining 2972 trajectories are provided in the dataset (data/retained_tracks.npy). [Notebook 3](3_make_tracked_videos.ipynb) can be used to make a video showing the retained trajectories, which "flash" as they cross the countline (Supplementary Video 2, provided in the figures_videos folder of the dataset).

**7. Generate video clips for manual review.** We use [Notebook 3](3_make_tracked_videos.ipynb) to create a "review video" (figure_videos/review_video.mp4 in the dataset) showing the retained tracks, which flash as they go over the countline, as well as vertical red lines dividing the video into horizontal sections. We then use [Notebook 5](5_make_validation_videos.ipynb) to split this video into 5 horizontal sections, each of which is then split into 20-second segments. The generated video clips are provided in the dataset (figures_videos/manual_review_clips).

**8. Manual review of tracking output.** Each of the video clips generated in Step 7 was manually reviewed by three reviewers who counted the number of animals missed or double-counted by the tracking procedure, as well as animals counted that were not buffalo (there was 1 zebra). Any clips for which all three reviewers did not agree then underwent a second round of review by a single reviewer. The accuracy of this second round was assessed using a double-observer procedure [see Notebook 6](6_double_observer.ipynb). The full results of the manual counting procedure are reported in the Supplement and main text.

**9. Forecasting of missed buffalo.** The drone video ended while buffalo were still entering the video frame. We thus fit a regression model to the count data and used this model to predict how many buffalo we may have missed due to the video ending early. This analysis is performed in [Notebook 7](7_decline_curve_analysis.ipynb).

Please see the manuscript and supplement for further details of this study.

**References**
1. Koger, B., Deshpande, A., Kerby, J.T., Graving, J.M., Costelloe, B.R., Couzin, I.D. (2023) Quantifying the movement, behavior and environmental context of group-living animals using drones and computer vision. *Journal of Animal Ecology* 92: 1351-1371.